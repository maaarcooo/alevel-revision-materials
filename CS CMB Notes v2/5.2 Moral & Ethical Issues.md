# 5.2 Moral & Ethical Issues

## Specification Overview

This topic covers the **moral, social, ethical and cultural opportunities and risks of digital technology**, including:
- Computers in the workforce
- Automated decision making
- Artificial intelligence
- Environmental effects
- Censorship and the Internet
- Monitor behaviour
- Analyse personal information
- Piracy and offensive communications
- Layout, colour paradigms and character sets

---

## Morals and Ethics: Key Definitions

| Term | Definition |
|------|------------|
| **Ethics** | Concerned with our **values as a community** and how these will impact different groups of people in society |
| **Morals** | To do with our **personal code of conduct** and encompass how we choose to behave, including the decisions we make at the expense of others |

With computers becoming an integral part of almost every aspect of our day-to-day lives, it is important to consider the moral, ethical, environmental, social and cultural implications of these changes. **Identifying these issues is the first step to resolving them.**

---

## 1. Computers in the Workforce

### Benefits of Computerisation

- **Improved efficiency**: Machines reduce delivery times and speed up manufacturing processes
- **Improved customer satisfaction** through faster service
- **Reduced unit labour costs** which feed through as lower prices for consumers
- **Reduced strain on workers** and reduced need to work in repetitive and tedious environments
- Rise in **online services** (online shopping, online banking) reducing costs of physical premises
- **Increased demand for computing-related occupations**: software developers, network engineers, web developers

### Risks and Concerns

- **Structural unemployment**: People losing jobs, particularly in middle-income manufacturing roles
- **Shift toward low-income service jobs** as manufacturing roles are replaced
- Jobs beyond manufacturing at risk: library assistants, taxi drivers (autonomous vehicles)
- **High dependence on computers**: If something goes wrong, major loss of output
- Need for **retraining programmes** to help workers change occupation
- Employers now require workers who can **confidently and productively use computers**

### Moral, Social, Ethical and Cultural Implications

| Category | Implications |
|----------|-------------|
| **Moral** | Privacy concerns about data collection and surveillance; cybersecurity responsibilities |
| **Social** | Digital divide creating inequality; impact on social interaction; work-life balance blur in remote work |
| **Ethical** | Job displacement concerns; algorithmic bias in hiring; intellectual property issues with digital copying |
| **Cultural** | Unequal access to cultural resources; need for content localisation; risk of reinforcing cultural stereotypes |

---

## 2. Automated Decision Making

### Definition

**Automated decision making** refers to the process of using **algorithms and computer systems to make decisions without direct human intervention**, based on data analysis, pattern recognition, and predefined rules.

### Applications

| Application | Description |
|-------------|-------------|
| **Social media feeds** | Determining what users see based on interactions and inferred interests |
| **Credit scoring** | Assessing creditworthiness and determining loan approvals |
| **Criminal justice** | Assessing likelihood of reoffending (e.g., COMPAS system in US) |
| **Job applications** | Screening candidates for desired qualities |
| **Medical diagnosis** | Analysing patient data to provide diagnoses and treatment recommendations |
| **Fraud detection** | Analysing transaction patterns to flag suspicious activities |
| **Driverless cars** | Making split-second decisions faster than humans can react |
| **Stock market trading** | Looking at past trends to model future changes |
| **Plant automation/power distribution** | Responding instantly to changes in demand |
| **Traffic management** | Optimising traffic flow and adjusting signal timings |

### Key Concerns

- Algorithms may **not consider extenuating circumstances**
- Unable to process information with the same **consideration of contextual factors** as humans
- Creates a **dangerous bubble** (filter bubble) where beliefs are never challenged
- People may be **treated unfairly** by algorithmic decisions
- **Accountability questions**: Who is responsible when things go wrong?
- Algorithms are designed by humans so **cannot be assumed to be free of bias**

### Requirements for Fair Automated Decision Making

1. Algorithms must be **thoroughly tested**
2. Must be **free of bias** against any group of people
3. Must be provided with **factually correct data**
4. **Algorithms should not remain a black box** - decisions must be justified
5. **Human input is essential** to ensure fairness
6. Need for **transparency and accountability**

### Moral, Social, Ethical and Cultural Implications

| Category | Implications |
|----------|-------------|
| **Moral** | Fairness and bias concerns; accountability and responsibility |
| **Social** | Lack of transparency erodes public trust; impact on employment; access to technology disparities |
| **Ethical** | Informed consent requirements; privacy and data collection concerns; algorithmic transparency |
| **Cultural** | May not account for cultural nuances; bias in cultural representation |

---

## 3. Artificial Intelligence

### Definition

**Artificial Intelligence (AI)** is the ability of a computer to:
- **Replicate human intelligence**
- Simulate **cognitive ability**
- **Grasp abstract concepts**
- Perform **learning, problem-solving, reasoning, perception, and decision-making**

### Key AI Systems

| System | Description |
|--------|-------------|
| **Expert Systems** (Knowledge-based systems) | Replicate knowledge and experience of an expert in a particular subject. Made up of a **knowledge base** (set of facts and rules) used to build an **inference engine** which is interrogated to find diagnoses |
| **Neural Networks** | Replicate biological neural networks; 'learn' from data sets and apply knowledge to new data; used in pattern detection and financial fraud detection |
| **Voice Recognition Systems** | Used in smart home systems (Google Home, Amazon Alexa); raise privacy concerns as they must be constantly switched on |

### Moral, Social, Ethical and Cultural Implications

| Category | Implications |
|----------|-------------|
| **Moral** | Developers face dilemmas about accountability, privacy, job displacement; must ensure systems prioritise ethical values and avoid biases; concerns about data privacy, security, and informed consent |
| **Social** | Changes in employment structures requiring reskilling; digital divide in AI accessibility; AI in healthcare raises questions about decision-making authority and liability |
| **Ethical** | Bias and fairness concerns if trained on biased datasets; autonomous systems raise ethical dilemmas (trolley problem in self-driving cars); data privacy and potential misuse |
| **Cultural** | Language models may uphold cultural biases; debates about authenticity of AI-generated art; AI can help preserve cultural heritage but requires cultural sensitivity |

### Key Ethical Questions

- **Accountability**: Who is responsible when things go wrong?
- **Sentience**: If AI ever reaches sentience, what rights should it have?
- **Bias**: How do we ensure AI systems don't perpetuate discrimination?

---

## 4. Environmental Effects

### Negative Environmental Impacts

| Issue | Description |
|-------|-------------|
| **E-waste** | Devices thrown away contain **mercury and radioactive isotopes** which are toxic and can contaminate water supplies |
| **E-waste disposal** | Often shipped to **third world countries with lower environmental standards** - considered immoral and unacceptable |
| **Increased electricity consumption** | Uses **non-renewable fossil fuels** which emit **greenhouse gases**, contributing to **global warming** |
| **Throw-away attitude** | Pressure to keep up with trends leads to more devices being discarded |
| **Impact on future generations** | Effects on planet will impact future generations and **biodiversity** |

### Positive Environmental Technologies

- Push for **renewable energy** in the UK
- **Smart home systems**: Temperature sensors for heating control, motion sensors to switch off lights
- **Sleep and Stand-by features** on computers and laptops
- Car engines designed to **prevent idling** to reduce emissions
- Technology has potential to **offset some environmental effects**

### E-waste Disposal Best Practices

1. **Reduce and Reuse**: Reduce consumption, repair devices, sell or donate functional devices
2. **Recycle**: Recover valuable materials for manufacturing new products
3. **E-Waste Recycling Programs**: Use manufacturer/retailer take-back programs
4. **Designated Facilities**: Dispose at facilities equipped to handle hazardous materials safely
5. **Follow Local Regulations**: Ensure legal and responsible disposal
6. **Data Security**: Erase all personal data before disposing

### Moral, Social, Ethical and Cultural Implications

| Category | Implications |
|----------|-------------|
| **Moral** | Environmental responsibility to protect planet for future generations; social justice concerns as lower-income communities bear disproportionate burden |
| **Social** | Environmental equity - fair distribution of benefits and burdens; digital divide in access to energy-efficient technologies |
| **Ethical** | Sustainability and stewardship; transparency and accountability for environmental practices; environmental justice |
| **Cultural** | Cultures prioritising preservation may conflict with e-waste practices; digitalisation raises questions about cultural identity |

---

## 5. Censorship and the Internet

### Definition

**Censorship** is the act of **suppressing the content that people are able to view, publish and access**. It involves control by governments, organisations, or ISPs to restrict access to specific websites, content, or platforms.

### Reasons for Censorship

| Reason | Description |
|--------|-------------|
| **National Security** | Prevent spread of sensitive information threatening security |
| **Restricting Harmful Content** | Block hate speech, violence encouragement, child exploitation |
| **Preserving Cultural Norms** | Some countries censor content contradicting cultural/religious beliefs |
| **Combating Disinformation** | Counter false or misleading information |
| **Terrorism/Extremism** | UK ISPs block websites associated with terrorism and extremist political beliefs |
| **Schools/Workplaces** | Prevent access to unsuitable material; maintain productivity |

### Arguments For and Against

| For Censorship | Against Censorship |
|----------------|-------------------|
| Necessary for national security | May be used to block alternative political beliefs |
| Filters offensive comments and extremist propaganda | Could push a certain ideology rather than protect |
| Protects vulnerable groups from harmful content | Restricts freedom of speech |
| Tim Berners-Lee created internet for sharing and mutual benefit | Creates a "Free Internet" principle violation |

### Moral, Social, Ethical and Cultural Implications

| Category | Implications |
|----------|-------------|
| **Moral** | Freedom of expression as fundamental human right; protecting vulnerable communities; accountability for content moderation decisions |
| **Social** | Impact on access to information and public discourse; censorship can exacerbate digital divide; affects social cohesion |
| **Ethical** | Privacy concerns with invasive surveillance; need for transparency and accountability; right to access accurate and diverse information |
| **Cultural** | Impact on cultural diversity and expression; preservation of cultural values; hindrance to cross-cultural dialogue |

---

## 6. Monitor Behaviour

### Methods of Monitoring

| Method | Description |
|--------|-------------|
| **CCTV cameras** | Surveillance for security and crime detection |
| **Tracking phone calls** | Monitoring communications |
| **GPS monitoring** | Tracking location (e.g., ankle monitors for house arrest) |
| **Email monitoring** | Employer monitoring of workplace communications |
| **Website/application tracking** | Employers tracking websites accessed and time spent |

### Purposes of Monitoring

- **Security and crime prevention**: Identifying threats, prompt responses
- **Public safety and deterrence**: Visible surveillance discourages criminal behaviour
- **Investigative purposes**: Gathering evidence in law enforcement
- **Emergency response**: Swift location of individuals needing assistance
- **Employee productivity**: Assessing adherence to company policies

### Ethical Debate

| Arguments For | Arguments Against |
|---------------|-------------------|
| Necessary to put people off committing crime | Unethical - contravenes basic human rights |
| Useful for tracing and punishing criminal activity | May lead to feelings of being constantly watched |
| Enhances security | Can impact public trust in institutions |
| Helps locate missing persons | Marginalised communities may be disproportionately affected |

### Moral, Social, Ethical and Cultural Implications

| Category | Implications |
|----------|-------------|
| **Moral** | Privacy vs security balance; individual autonomy concerns; clear justification needed |
| **Social** | Impact on public trust in institutions; may promote self-censorship; disproportionate impact on vulnerable groups |
| **Ethical** | Data security and privacy protection requirements; transparency and accountability; purpose limitation |
| **Cultural** | Must respect cultural values and traditions; potential use for cultural preservation; need for diversity and inclusion |

---

## 7. Analyse Personal Information

### Key Concepts

| Term | Definition |
|------|------------|
| **Big Data** | Large amounts of data from a number of sources |
| **Data Mining** | Process of analysing big data to make inferences about people's behaviour, likes and dislikes |

### Uses of Personal Information Analysis

- **Targeted advertising**
- **Personalised political propaganda**
- **Identifying unknown connections** between variables
- **Healthcare**: Predicting disease trends, personalising treatment plans
- **Fraud detection**: Pattern recognition in financial transactions

### Technologies Used

| Technology | Description |
|------------|-------------|
| **Data Mining** | Extracting patterns from large datasets |
| **Machine Learning** | Predicting outcomes, assessing risks, personalising treatment based on past data |
| **Artificial Intelligence** | Processing large data amounts for predictions and recommendations |

### Concerns

- Insights could **act against certain individuals** (e.g., insurance companies denying coverage based on disease risk)
- Companies may not be **transparent about data collection**
- Data not always obvious to users
- Pressure for companies to be **more transparent**

### Relevant Legislation

- **Data Protection Act 1998** - no longer sufficient for current digital landscape
- **GDPR (May 2018)** - enforced to better protect public welfare
- Companies legally required to **state what kinds of data they collect**

### Moral, Social, Ethical and Cultural Implications

| Category | Implications |
|----------|-------------|
| **Moral** | Informed consent is crucial; equitable distribution of benefits |
| **Social** | Privacy breach risks; digital divide may exclude those without technology access |
| **Ethical** | Data security obligations; transparency and accountability; robust auditing mechanisms |
| **Cultural** | Cultural sensitivity in data use; fair representation of all cultural groups to avoid biased outcomes |

---

## 8. Piracy and Offensive Communications

### Piracy

**Definition**: The **unauthorised copying of content**, such as software or media including music and films. This is a **form of theft and is illegal**.

| Type | Description |
|------|-------------|
| **Software Piracy** | Unauthorised copying, distribution, or use of copyrighted software (counterfeiting, internet piracy, end-user piracy) |
| **Music and Film Piracy** | Unauthorised copying and distribution through peer-to-peer networks or illegal download websites |
| **eBook Piracy** | Unauthorised distribution of digital books |

### Moral, Social, Ethical and Cultural Implications of Piracy

| Category | Positive View | Negative View |
|----------|---------------|---------------|
| **Moral** | Can democratise access to content in economically restricted regions | Direct infringement on intellectual property rights |
| **Social** | Encourages sharing and dissemination of knowledge | Inflicts financial losses on creators and industries; leads to job losses |
| **Ethical** | Highlights gaps in legal access, prompting discussion of affordable distribution | Fosters culture of dishonesty and unfair practices |
| **Cultural** | Increases global exposure for cultural products | Disproportionately affects creators from marginalised cultures |

### Offensive Communications

**Definition**: Any sort of **online harassment**, including **cyber-bullying or stalking**. The Internet provides a **seemingly anonymous front** for people.

### Key Legislation

| Law | Description |
|-----|-------------|
| **Malicious Communications Act 1988** | Makes it a **criminal offence to send indecent or offensive messages to anyone online**; can be traced by law enforcement and result in criminal record |

---

## 9. Layout, Colour Paradigms and Character Sets

### Layout Considerations

| Consideration | Description |
|---------------|-------------|
| **Target Audience** | Web developers must consider who will be viewing the website |
| **Navigation** | Menus must make it easy to navigate between pages |
| **Reading Direction** | English-speaking countries: menus on left; Arabic-speaking countries (Egypt, UAE): menus on right (Arabic reads right to left) |
| **Bi-directional Text** | Programs must support both left-to-right and right-to-left reading directions |
| **Accessibility** | Must accommodate people with visual impairments or disabilities |
| **Legal Requirements** | **Equality Act 2010** makes it illegal to discriminate against providing a service to certain groups |

### Accessibility Features

- Ability to **enlarge text**
- Ability to **alter contrast**
- **Alternative text (alt text)** for images
- **Screen magnifier options**
- **Transcripts of audio files** for hearing impairments

### Colour Paradigms

Web developers must consider **how different colours are interpreted around the world**.

| Colour | Western Interpretation | Other Interpretations |
|--------|------------------------|----------------------|
| **White** | Purity | Mourning (Middle East) |
| **Red** | Danger, urgency | Luck (Chinese culture) |
| **Green** | Luck, nature | Generally positive connotations globally |
| **Blue** | Trust, stability | Generally positive |

**Best Practice**: Choose **neutral colour schemes with widely positive connotations** (e.g., green).

### Character Sets

| Character Set | Description |
|---------------|-------------|
| **ASCII** | Uses 7 bits; unable to represent all characters in languages like Chinese |
| **Unicode** | Preferred character set; able to represent **over a million characters**; supports most of the world's writing systems |

### Why Unicode is Preferred

- Websites must be **translated into multiple languages**
- Some character sets are too small to accommodate all characters
- Unicode ensures **languages are accurately represented**
- Enables websites to be **accessible to as wide an audience as possible**
- Allows applications to be **localised** (adapted to different languages and regions without engineering changes)

---

## Summary: Key Exam Points

### For Discussion Questions

When discussing moral, social, ethical and cultural implications:

1. **Moral**: Personal conduct, responsibility, right vs wrong
2. **Social**: Impact on society, communities, groups of people
3. **Ethical**: Community values, professional standards, fairness
4. **Cultural**: Traditions, heritage, different cultural perspectives

### Common Themes Across All Topics

- **Privacy vs Security** balance
- **Transparency and Accountability** requirements
- **Digital Divide** concerns
- **Bias and Fairness** in algorithms
- **Environmental Responsibility**
- **Accessibility and Inclusion**
- **Global/Cultural Considerations**
- **Legal Compliance** (GDPR, DPA, Equality Act, Malicious Communications Act)

### Key Legislation to Remember

| Law | Year | Purpose |
|-----|------|---------|
| Data Protection Act | 1998 | Data protection (superseded by GDPR) |
| Malicious Communications Act | 1988 | Criminalises offensive online messages |
| Equality Act | 2010 | Prevents discrimination in service provision |
| GDPR | 2018 | Enhanced data protection and privacy |
