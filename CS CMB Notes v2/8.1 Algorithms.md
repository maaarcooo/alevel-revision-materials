# 8.1 Algorithms

## Analysis of Algorithms

When analysing an algorithm, there are two key properties to consider:

- **Time Complexity** - how much time an algorithm requires to solve a problem
- **Space Complexity** - how much storage/memory an algorithm requires

---

## Time Complexity and Big-O Notation

**Time complexity** measures how the execution time of an algorithm grows relative to the input size. It is expressed using **Big-O notation**, which shows an **upper limit** for the amount of time taken relative to the number of data elements given as input.

Big-O notation allows you to **predict** the amount of time it takes for an algorithm to finish given the number of data elements.

### Common Time Complexities

| Big-O Notation | Name | Description |
|----------------|------|-------------|
| **O(1)** | Constant | Time taken is **independent** from the number of elements inputted |
| **O(log n)** | Logarithmic | Time increases at a **smaller rate** as input size increases (e.g., divide and conquer algorithms) |
| **O(n)** | Linear | Time is **directly proportional** to the number of elements inputted |
| **O(n log n)** | Linearithmic | Common in efficient sorting algorithms |
| **O(n²)** | Polynomial (Quadratic) | Time is proportional to the **square** of input size (e.g., nested loops) |
| **O(2ⁿ)** | Exponential | Time **doubles** with every additional item |

### Efficiency Ranking (Best to Worst)

```
O(1) → O(log n) → O(n) → O(n log n) → O(n²) → O(2ⁿ)
Best                                           Worst
```

### Determining Time Complexity

- **Single operation** (e.g., `print("hello")`) → **O(1)** constant time
- **Single loop** iterating n times → **O(n)** linear time
- **Nested loops** (loop within a loop) → **O(n²)** polynomial time
- **Divide and conquer** (halving data each iteration) → **O(log n)** logarithmic time
- **Recursive algorithms** solving two subproblems of size n-1 → **O(2ⁿ)** exponential time

### Logarithms

A **logarithm** is the **inverse of an exponential** - it determines how many times a base number must be multiplied by itself to reach another number.

| x | y = log₂(x) |
|---|-------------|
| 1 (2⁰) | 0 |
| 8 (2³) | 3 |
| 1024 (2¹⁰) | 10 |

---

## Space Complexity

**Space complexity** is the amount of **storage** an algorithm requires, also expressed using Big-O notation.

- Algorithms store extra data whenever they **make copies**
- When working with large datasets, making copies is **expensive** in terms of storage
- To **reduce space complexity**, perform operations on the **original data** rather than creating copies

### Trade-offs

There is often a trade-off between time and space complexity. The priority depends on the situation:

- **Limited storage but fast processing** → Focus on space complexity
- **Large data requiring quick processing** → Focus on time complexity

---

## Searching Algorithms

Searching algorithms are used to **find a specified element** within a data structure.

### Linear Search

**Linear search** is the most basic searching algorithm that checks each element **one by one** sequentially until the desired element is found.

**Time Complexity:** **O(n)**

**Characteristics:**
- **Easy to implement**
- **Does not require sorted data**
- Performance depends on luck - element might be found immediately or at the end
- Inefficient for large datasets

**Pseudocode:**
```
A = Array of data
x = Desired element

i = 0
while i < A.length:
    if A[i] == x:
        return i
    else:
        i = i + 1
    endif
endwhile
return "Not found in data"
```

### Binary Search

**Binary search** is a **divide and conquer** algorithm that can only be applied on **sorted data**. It works by finding the **middle element** and deciding which half of the data contains the desired element, then **discarding** the unwanted half.

**Time Complexity:** **O(log n)**

**Requirements:**
- Data must be **sorted**

**How it works:**
1. Find the **middle element**
2. If middle element equals target, return position
3. If middle element > target, search **left half** (set high = mid - 1)
4. If middle element < target, search **right half** (set low = mid + 1)
5. Repeat until found or low > high (not found)

**Pseudocode:**
```
A = Array of data
x = Desired element

low = 0
high = A.length - 1

while low <= high:
    mid = (low + high) / 2
    if A[mid] == x:
        return mid
    else if A[mid] > x:
        high = mid - 1
    else:
        low = mid + 1
    endif
endwhile
return "Not found in data"
```

### Comparison of Searching Algorithms

| Feature | Linear Search | Binary Search |
|---------|--------------|---------------|
| Time Complexity | O(n) | O(log n) |
| Requires Sorted Data | No | Yes |
| Implementation | Simple | More complex |
| Efficiency | Less efficient | More efficient |

---

## Sorting Algorithms

**Sorting algorithms** take elements in **any order** and output them in a **logical order** (usually numerical or **lexicographic** - phonebook style ordering).

Most sorting algorithms output in **ascending order**, but can be modified for descending order by switching the inequality or reversing the output.

### Bubble Sort

**Bubble sort** makes **comparisons and swaps** between pairs of elements. The largest element "**bubbles**" to the top of the data with each pass.

**Time Complexity:** **O(n²)**

**How it works:**
1. Compare adjacent pairs of elements starting from the beginning
2. If they are in the **wrong order**, **swap** them
3. Continue until the end of the array (this is one **pass**)
4. Repeat passes until the array is sorted
5. After each pass, the largest unsorted element is in its correct position

**Basic Pseudocode:**
```
A = Array of data

for i = 0 to A.length - 1:
    for j = 0 to A.length - 2:
        if A[j] > A[j+1]:
            temp = A[j]
            A[j] = A[j+1]
            A[j+1] = temp
        endif
return A
```

**Optimised Bubble Sort:**
```
A = Array of data

for i = 0 to A.length - 1:
    noSwap = True
    for j = 0 to A.length - (i + 1):
        if A[j] > A[j+1]:
            temp = A[j]
            A[j] = A[j+1]
            A[j+1] = temp
            noSwap = False
    if noSwap:
        break
return A
```

**Optimisations:**
- **Flag** (`noSwap`) - if a complete pass occurs with **no swaps**, the algorithm terminates early
- **Reducing comparisons** - after each pass, one more element is in place, so fewer comparisons are needed

**Key Point:** The algorithm terminates after a complete pass with **no swaps**, not just when elements appear sorted.

### Insertion Sort

**Insertion sort** builds a **sorted sequence** by inserting each element into its correct position within the already-sorted portion.

**Time Complexity:** **O(n²)**

**Important:** In the iᵗʰ iteration, the first i elements are sorted, but they are **not necessarily the i smallest elements** in the input.

**How it works:**
1. Start at the **second element** (first element is already a sorted sequence of 1)
2. Compare it to elements on its **left**
3. **Insert** it into the correct position in the sorted portion by shifting elements right
4. Move to the next element and repeat
5. Continue until all elements are inserted into the sorted sequence

**Pseudocode:**
```
A = Array of data

for i = 1 to A.length - 1:
    elem = A[i]
    j = i - 1
    while j >= 0 and A[j] > elem:
        A[j+1] = A[j]
        j = j - 1
    A[j+1] = elem
```

### Merge Sort

**Merge sort** is a **divide and conquer** algorithm that recursively divides the input until individual elements are isolated, then **merges** them back together in sorted order.

**Time Complexity:** **O(n log n)**

**Consists of two functions:**
1. **MergeSort** - divides the input into two parts recursively
2. **Merge** - combines two sorted lists into one sorted list

**How it works:**
1. **Divide** the array in half recursively until each subarray has length 1
2. **Merge** adjacent subarrays by comparing first elements and placing the smaller one in a new list
3. Repeat merging until one fully sorted array remains

**Pseudocode:**
```
A = Array of data

MergeSort(A)
    if A.length <= 1:
        return A
    else:
        mid = A.length / 2
        left = A[0...mid]
        right = A[mid+1...A.length-1]
        leftSort = MergeSort(left)
        rightSort = MergeSort(right)
        return Merge(leftSort, rightSort)
```

**Note:** The exact implementation of Merge is not required, but understanding how it works is.

### Quick Sort

**Quick sort** selects a **pivot** element and **divides** the input around it. Elements smaller than the pivot go to the left; larger elements go to the right.

**Time Complexity:** **O(n²)** worst case

**Note:** Despite its name, quick sort's worst-case time complexity is O(n²), which occurs when the pivot consistently results in unbalanced partitions.

**How it works:**
1. Select a **pivot** (often the central element)
2. **Partition**: place elements smaller than pivot to the left, larger to the right
3. The pivot is now in its **correct final position**
4. **Recursively** apply to the left and right sublists
5. Continue until all elements are old pivots or lists of length 1

**Pivot Selection:**
- When there is no central element, the pivot is typically **rounded up**
- Once a pivot is placed, it **never moves** - it is always in the correct position

**Termination Condition:** Every element is either an old pivot or forms a list of length 1.

### Comparison of Sorting Algorithms

| Algorithm | Time Complexity | Space Complexity | Notes |
|-----------|----------------|------------------|-------|
| Bubble Sort | O(n²) | O(1) | Simple but slow; can terminate early if optimised |
| Insertion Sort | O(n²) | O(1) | Efficient for small or nearly sorted data |
| Merge Sort | O(n log n) | O(n) | Consistent performance; uses divide and conquer |
| Quick Sort | O(n²) worst case | O(log n) | Uses divide and conquer; performance depends on pivot selection |

---

## Path-Finding Algorithms

Path-finding algorithms find the **shortest path** between nodes in a **weighted graph**.

### Dijkstra's Algorithm

**Dijkstra's algorithm** finds the **shortest path between two nodes** in a **weighted graph**.

**Implementation:** Commonly uses a **priority queue** with smallest distances at the front.

**Applications:**
- Network routing (finding optimal packet forwarding routes)
- GPS navigation
- Any scenario requiring shortest path calculation

**How it works:**
1. Set distance to **start node as 0**, all other nodes as **∞**
2. Add all immediately neighbouring nodes to the priority queue with their distances
3. Remove the node with the **smallest distance** from the queue
4. For each unvisited neighbour:
   - Calculate distance through current node
   - If smaller than recorded distance, **update** it
5. Mark current node as visited
6. Repeat until destination is reached or all nodes visited
7. **Trace back** through the table to find the shortest path

**Table Method:**
| Node | From | Distance | Total Distance |
|------|------|----------|----------------|
| B | A | 7 | 7 |
| C | A | 3 | **3** ← smallest, select next |

**Key Points:**
- Always selects the node with the **smallest total distance** next
- Once visited, a node's shortest distance is final
- Works with any weighted graph
- Guaranteed to find the shortest path

### A* Algorithm

The **A* Algorithm** is an improvement of Dijkstra's algorithm that uses **two cost functions**:

1. **Actual cost (g)** - the same cost measured in Dijkstra's algorithm (distance from start to current node)
2. **Heuristic cost (h)** - an **approximate cost** from current node to the final node

**Total cost: f = g + h**

**Heuristic:**
- An **estimate** of the distance to the goal
- Often calculated using **trigonometry** (e.g., straight-line distance)
- Must be **admissible** (never overestimate the actual cost) to guarantee optimal solution

**How it differs from Dijkstra's:**
- A node with a **higher actual cost** may be chosen if it has a **lower total cost** (including heuristic)
- This aims to **reduce the total time** taken to find the shortest path
- Effectiveness depends on the **accuracy of the heuristics**

**Table Method (with Heuristic):**
| Node | From | Distance | Heuristic | Total Distance |
|------|------|----------|-----------|----------------|
| B | A | 7 | 6 | 13 |
| D | A | 6 | 1 | **7** ← smallest, select next |

**Key Points:**
- The heuristic cost of the **previous node** is **not** added to the new distance
- More efficient than Dijkstra's when good heuristics are available
- If heuristic = 0 for all nodes, A* behaves like Dijkstra's algorithm

### Comparison of Path-Finding Algorithms

| Feature | Dijkstra's Algorithm | A* Algorithm |
|---------|---------------------|--------------|
| Cost Functions | Actual cost only | Actual cost + Heuristic |
| Efficiency | Explores more nodes | More efficient with good heuristics |
| Guaranteed Optimal | Yes | Yes (if heuristic is admissible) |
| Best Use Case | Unknown destination or no good heuristic | Known destination with good distance estimate |

---

## Summary of Algorithm Time Complexities

| Algorithm | Best Case | Average Case | Worst Case |
|-----------|-----------|--------------|------------|
| **Linear Search** | O(1) | O(n) | O(n) |
| **Binary Search** | O(1) | O(log n) | O(log n) |
| **Bubble Sort** | O(n) | O(n²) | O(n²) |
| **Insertion Sort** | O(n) | O(n²) | O(n²) |
| **Merge Sort** | O(n log n) | O(n log n) | O(n log n) |
| **Quick Sort** | O(n log n) | O(n log n) | O(n²) |

---

## Exam Tips

1. **Know pseudocode** for all algorithms - you may be asked to trace through them
2. **Understand time complexity** - be able to identify Big-O from code structure
3. **Compare algorithms** - know advantages and disadvantages of each
4. **Trace through examples** - practice working through sorting/searching on sample data
5. For **path-finding**, use tables to track nodes, distances, and routes
6. Remember **binary search requires sorted data**
7. For **A***, remember the heuristic is only added to the current node's calculation
8. **Bubble sort** terminates after a pass with no swaps, not when data appears sorted
9. **Insertion sort** - sorted elements are not necessarily the smallest elements
10. **Merge sort** creates new lists (higher space complexity) but consistent O(n log n) time
