# 1.1 Structure & Function of the Processor

## Components of the CPU

The **CPU** (Central Processing Unit) is responsible for processing all data and executing instructions within the computer. It consists of several key components.

### Arithmetic and Logic Unit (ALU)

The **ALU** performs:

- **Arithmetic operations**: Mathematical calculations such as addition, subtraction, multiplication, and division on fixed or floating-point numbers
- **Logical operations**: Boolean logic operations including AND, OR, NOT, and XOR
- **Comparison operations**: Evaluating conditions for decision-making

The ALU contains several sub-components including arithmetic circuits, logic circuits, internal registers, status flags (e.g. overflow flag, zero flag), and internal buses.

### Control Unit (CU)

The **Control Unit** directs the operations of the CPU. Its functions include:

- Controlling and coordinating CPU activities
- Managing data flow between the CPU and other devices
- Fetching the next instruction from memory
- Decoding instructions to determine required operations
- Generating timing signals to synchronise operations
- Storing resulting data back in memory

### Registers

**Registers** are small, high-speed memory cells within the CPU used to temporarily store data. All arithmetic, logical, and shift operations occur in registers.

| Register | Purpose |
|----------|---------|
| **Program Counter (PC)** | Holds the address of the next instruction to be fetched |
| **Accumulator (ACC)** | Stores results from calculations and values being processed |
| **Memory Address Register (MAR)** | Holds the address of a memory location to be read from or written to |
| **Memory Data Register (MDR)** | Temporarily stores data that has been read from memory or data to be written to memory |
| **Current Instruction Register (CIR)** | Holds the current instruction being executed, split into opcode and operand |

## System Buses

**Buses** are sets of parallel wires connecting components inside the CPU and between the CPU and memory. The three buses collectively form the **system bus**.

**Bus width** refers to the number of parallel wires. A wider bus can transfer more bits simultaneously. Common widths are 8, 16, 32, or 64 bits.

### Data Bus

- **Bi-directional** bus (carries data in both directions)
- Transports data and instructions between components
- Width determines how much data can be transferred per cycle

### Address Bus

- **Unidirectional** bus (CPU sends addresses to memory)
- Transmits memory addresses specifying where data should be sent to or retrieved from
- Width determines the number of **addressable memory locations** (e.g. 32-bit address bus can address 2³² locations)

### Control Bus

- **Bi-directional** bus
- Transmits control signals between internal and external components
- Coordinates use of address and data buses
- Provides status information between system components

**Control signals include:**

- **Bus request**: Device requesting use of the data bus
- **Bus grant**: CPU has granted access to the data bus
- **Memory write**: Data to be written to addressed location
- **Memory read**: Data to be read from specified location onto data bus
- **Interrupt request**: Device requesting CPU attention
- **Clock signal**: Synchronises all operations

## Assembly Language and Machine Code

**Assembly language** uses **mnemonics** (e.g. ADD, SUB, LDA) to represent instructions in a human-readable form. This is a simplified representation of **machine code**.

Each instruction in the CIR is divided into:

- **Opcode**: Specifies the type of instruction to be executed
- **Operand**: Contains the data value or the address of the data upon which the operation is performed

The **addressing mode** (part of the opcode) specifies how the operand should be interpreted.

## Fetch-Decode-Execute Cycle

The **fetch-decode-execute cycle** (FDE cycle) is the sequence of operations the CPU performs continuously to execute instructions.

### Fetch Phase

1. Address from PC is copied to the MAR
2. Address sent via address bus; memory read signal sent via control bus
3. Instruction at that address is copied to MDR via the data bus
4. Simultaneously, PC is incremented by 1
5. Value in MDR is copied to the CIR

### Decode Phase

1. Contents of CIR are split into opcode and operand
2. Instruction is sent to the Control Unit to be decoded
3. CU determines what operation is required

### Execute Phase

1. The decoded instruction is executed
2. May involve: ALU operations, data transfer to/from memory, or modifying the PC (for branch instructions)
3. Results may be stored in the ACC or written to memory

## Factors Affecting CPU Performance

### Clock Speed

- The **system clock** generates regular electrical pulses (switching between 0 and 1)
- All CPU operations begin on a **clock pulse**
- **Clock speed** measures how many cycles occur per second (measured in Hz)
- 1 Hz = 1 cycle per second; typical modern CPUs run at several GHz (billions of cycles per second)
- Higher clock speed = more instructions executed per second = faster processing

### Number of Cores

- A **core** is an independent processing unit capable of running its own fetch-decode-execute cycle
- **Multi-core processors** (dual, quad, octa-core) can execute multiple instructions simultaneously via **parallel processing**
- Doubling cores does not double performance due to:
  - Overhead in coordinating tasks between cores
  - Not all programs are designed to utilise multiple cores efficiently
  - Some tasks cannot be parallelised

### Cache Memory

**Cache** is high-speed memory located within or very close to the CPU. It stores frequently used data and instructions for faster access than RAM.

| Cache Level | Properties |
|-------------|------------|
| **Level 1 (L1)** | Fastest, smallest capacity (2-64 KB), one per core |
| **Level 2 (L2)** | Fast, medium capacity (256 KB - 2 MB), may be shared between cores |
| **Level 3 (L3)** | Larger, slower than L1/L2, shared across all cores, sits on motherboard |

When cache fills up, least recently used data is replaced with new data.

## Pipelining (HT)

**Pipelining** is the technique of executing multiple instructions concurrently by overlapping the fetch, decode, and execute stages of different instructions.

| Step | Fetch | Decode | Execute |
|------|-------|--------|---------|
| 1 | Instruction A | - | - |
| 2 | Instruction B | Instruction A | - |
| 3 | Instruction C | Instruction B | Instruction A |
| 4 | Instruction D | Instruction C | Instruction B |

**Benefits:**

- Reduces CPU idle time
- Increases throughput (instructions completed per unit time)
- All parts of the processor can be utilised simultaneously

**Types:**

- **Instruction pipelining**: Overlapping fetch, decode, and execute stages
- **Arithmetic pipelining**: Breaking down and overlapping arithmetic operations

**Limitation:** When a **branch** instruction occurs, the pipeline must be **flushed** (cleared), as pre-fetched instructions may no longer be valid.

## Computer Architecture

### Von Neumann Architecture

Based on the **stored program concept**, where both data and instructions are stored in the same memory.

**Characteristics:**

- Single memory unit for both data and instructions
- Single set of buses (data, address, control) shared for all transfers
- Single Control Unit, ALU, and set of registers
- Instructions and data fetched sequentially over the same bus

**Advantages:**

- Simpler and cheaper to design (easier control unit)
- Programs can be optimised in size
- Flexible—same memory can hold varying amounts of data vs. instructions

### Harvard Architecture

Uses **physically separate memory** and **separate buses** for data and instructions.

**Characteristics:**

- Separate memory for instructions and data
- Separate buses for instruction and data transfers
- Data and instructions can be fetched simultaneously (parallel access)
- Commonly used in embedded processors and digital signal processors (DSPs)

**Advantages:**

- Faster execution—data and instructions fetched in parallel
- Memories can have different sizes optimised for their purpose
- Instruction memory can be read-only while data memory is read-write

| Von Neumann | Harvard |
|-------------|---------|
| Cheaper to develop | Faster execution (parallel fetching) |
| Programs can be optimised in size | Memory sizes can differ, making more efficient use of space |
| Simpler design | Better suited for real-time processing |

### Contemporary Processor Architecture

Modern processors use a **hybrid approach** combining both architectures:

- **Von Neumann** principles for main memory (unified data and instructions in RAM)
- **Harvard** principles for cache (separate instruction cache and data cache)

**Additional features in contemporary processors:**

- **Multiple cores**: Independent processing units for parallel execution
- **Hyperthreading/Virtual cores**: Physical core acts as two virtual cores
- **Out-of-order execution**: Instructions executed when ready, not strictly in order
- **Superscalar architecture**: Multiple instructions executed simultaneously
- **Branch prediction**: Predicting branch outcomes to reduce pipeline flushes
- **Performance boost mode**: Temporarily increasing clock speed when needed

## Exam Tips

- When describing the FDE cycle, be specific about which registers are used at each stage and what values they hold
- For questions about registers, always state both the register name and its specific contents based on the scenario
- Remember that pipelining is flushed on branches—this is a common exam point
- Distinguish clearly between Von Neumann (shared memory/buses) and Harvard (separate memory/buses)
- Cache questions often require you to explain the trade-off between speed and capacity across L1/L2/L3
